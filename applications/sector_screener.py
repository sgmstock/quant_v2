#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¿å—ç­›é€‰å™¨ (quant_v2 ç‰ˆæœ¬)

åŠŸèƒ½ï¼š
1. ç­›é€‰ç¬¦åˆæ¡ä»¶çš„æ¿å—
2. è®¡ç®—ç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…
3. å¤šæ—¶é—´æ®µåˆ†æ
"""
import pandas as pd
import numpy as np
import glob
import sqlite3
from datetime import datetime, timedelta
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)




def get_qualified_sectors(excel_path=None):
    if excel_path is None:
        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„æŒ‡å‘ databases ç›®å½•
        current_dir = os.path.dirname(os.path.abspath(__file__))
        excel_path = os.path.join(current_dir, '..', 'databases', 'sector_market_cap_analysis.xlsx')
        excel_path = os.path.abspath(excel_path)
    """
    ç­›é€‰ç¬¦åˆæ¡ä»¶çš„æ¿å—ï¼Œè¿”å›index_codeå’Œindex_name
    
    ç­›é€‰æ¡ä»¶ï¼š
    1. (å›½ä¼ > 0.4) & (è¶…å¼º > 0.7)
    2. æˆ–è€… è¶…è¶…å¼º > 0.6
    3. æˆ–è€… å¤§é«˜ > 0.8
    
    Args:
        excel_path (str): Excelæ–‡ä»¶è·¯å¾„
        
    Returns:
        tuple: (index_codes, index_names) ä¸¤ä¸ªåˆ—è¡¨
               - index_codes: ç¬¦åˆæ¡ä»¶çš„index_codeåˆ—è¡¨ï¼Œåç¼€å·²ä».SIæ”¹ä¸º.ZS
               - index_names: å¯¹åº”çš„æ¿å—åç§°åˆ—è¡¨
    """
    # è¯»å–Excelæ–‡ä»¶
    df = pd.read_excel(excel_path)
    
    # ç­›é€‰ç¬¦åˆæ¡ä»¶çš„æ¿å—
    # ä¿®å¤åŸæ¥çš„é€»è¾‘é”™è¯¯ï¼šä½¿ç”¨ | è€Œä¸æ˜¯ orï¼Œå¹¶æ­£ç¡®ä½¿ç”¨æ‹¬å·
    df_zhuli = df[
        ((df['å›½ä¼'] > 0.4) & (df['è¶…å¼º'] > 0.7)) | 
        (df['è¶…è¶…å¼º'] > 0.6) | 
        (df['å¤§é«˜'] > 0.8)
    ]
    
    # è·å–index_codeå¹¶å°†åç¼€ä».SIæ”¹ä¸º.ZS
    index_codes = df_zhuli['index_code'].str.replace('.SI', '.ZS', regex=False).tolist()
    
    # è·å–å¯¹åº”çš„æ¿å—åç§°ä½œä¸ºindex_name
    index_names = df_zhuli['æ¿å—åç§°'].tolist()
    
    return index_codes, index_names

def get_qualified_sector_codes(excel_path=None):
    if excel_path is None:
        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„æŒ‡å‘ databases ç›®å½•
        current_dir = os.path.dirname(os.path.abspath(__file__))
        excel_path = os.path.join(current_dir, '..', 'databases', 'sector_market_cap_analysis.xlsx')
        excel_path = os.path.abspath(excel_path)
    """
    ç­›é€‰ç¬¦åˆæ¡ä»¶çš„æ¿å—ï¼Œåªè¿”å›index_codeåˆ—è¡¨
    
    Args:
        excel_path (str): Excelæ–‡ä»¶è·¯å¾„
        
    Returns:
        list: ç¬¦åˆæ¡ä»¶çš„index_codeåˆ—è¡¨ï¼Œåç¼€å·²ä».SIæ”¹ä¸º.ZS
    """
    index_codes, _ = get_qualified_sectors(excel_path)
    return index_codes

# #é¢„è®¾è¿‘æœŸæ¶ˆæ¯åšå¼ˆåŸºæœ¬é¢çš„æ¿å—ï¼šæ–¹ä¾¿å¤–éƒ¨è°ƒç”¨ã€‚
# jinqi_xiaoxi_bankuai = []
# #é¢„è®¾é•¿çº¿å¼ºè¶‹åŠ¿æ¿å—ã€‚è°ƒç”¨æ•°æ®è¡¨ã€‚
# cxqqs_bankuai = []

# #ä¸»åŠ›å¼ºçš„ï¼Œç›´æ¥è®¡ç®—æ¯”ä¾‹ï¼ˆåœ¨æ¿å—è¯„åˆ†é‡Œé¢æœ‰äº†ï¼‰
# zhuli_bankuai = get_qualified_sector_codes()  # åªè·å–index_codes

#å†å²é•¿çº¿å¾ªç¯å±…å‰ï¼šç›´æ¥å¤–éƒ¨è°ƒç”¨å‡½æ•°
# changxian_bankuai = get_changxian_zf_bankuai()

#å±äºå†å²é•¿çº¿å¾ªç¯å±…å‰çš„æ¿å—
#databases\xunhuan_changxian_zf\*.csv
def get_changxian_zf_bankuai():
    """
    è¯»å–databases/xunhuan_changxian_zf/æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶ï¼Œè·å–index_codeåˆ—è¡¨
    è¿”å›æ‰€æœ‰é•¿çº¿æ¶¨å¹…å±…å‰æ¿å—çš„index_code
    """
    import os
    import glob
    
    # è¯»å–æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶
    csv_folder = r"databases\xunhuan_changxian_zf"
    csv_files = glob.glob(os.path.join(csv_folder, "*.csv"))
    
    if not csv_files:
        return []
    
    all_index_codes = []
    
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file, encoding='utf-8-sig')
            if 'index_code' in df.columns:
                # è·å–è¯¥æ–‡ä»¶ä¸­çš„æ‰€æœ‰index_code
                index_codes = df['index_code'].tolist()
                all_index_codes.extend(index_codes)
        except Exception as e:
            continue
    
    # å»é‡å¹¶è¿”å›
    unique_index_codes = list(set(all_index_codes))
    return unique_index_codes






#å±äºæœ€è¿‘3ä¸ªæ³¢æ®µå¾ªç¯å±…å‰çš„æ¿å—
#databases\xunhuan_boduan_zf\*.csv
def get_boduan_zf_bankuai():
    """
    è¯»å–databases/xunhuan_boduan_zf/æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶ï¼Œè·å–index_codeåˆ—è¡¨
    è¿”å›æ‰€æœ‰é•¿çº¿æ¶¨å¹…å±…å‰æ¿å—çš„index_code
    """
    import os
    import glob
    
    # è¯»å–æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶
    csv_folder = r"databases\xunhuan_boduan_zf"
    csv_files = glob.glob(os.path.join(csv_folder, "*.csv"))
    
    if not csv_files:
        return []
    
    all_index_codes = []
    
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file, encoding='utf-8-sig')
            if 'index_code' in df.columns:
                # è·å–è¯¥æ–‡ä»¶ä¸­çš„æ‰€æœ‰index_code
                index_codes = df['index_code'].tolist()
                all_index_codes.extend(index_codes)
        except Exception as e:
            continue
    
    # å»é‡å¹¶è¿”å›
    unique_index_codes = list(set(all_index_codes))
    return unique_index_codes

#å±äºæœ€è¿‘3ä¸ªæ³¢æ®µå¾ªç¯å±…å‰çš„æ¿å—
#databases\xunhuan_boduan_bias\*.csv
def get_boduan_bias_bankuai():
    """
    è¯»å–databases/xunhuan_boduan_bias/æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶ï¼Œè·å–index_codeåˆ—è¡¨
    è¿”å›æ‰€æœ‰é•¿çº¿æ¶¨å¹…å±…å‰æ¿å—çš„index_code
    """
    import os
    import glob
    
    # è¯»å–æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶
    csv_folder = r"databases\xunhuan_boduan_bias"
    csv_files = glob.glob(os.path.join(csv_folder, "*.csv"))
    
    if not csv_files:
        return []
    
    all_index_codes = []
    
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file, encoding='utf-8-sig')
            if 'index_code' in df.columns:
                # è·å–è¯¥æ–‡ä»¶ä¸­çš„æ‰€æœ‰index_code
                index_codes = df['index_code'].tolist()
                all_index_codes.extend(index_codes)
        except Exception as e:
            continue
    
    # å»é‡å¹¶è¿”å›
    unique_index_codes = list(set(all_index_codes))
    return unique_index_codes


def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    db_path = 'databases/quant_system.db'
    if not os.path.exists(db_path):
        raise FileNotFoundError(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
    return sqlite3.connect(db_path)


def get_sw_index_codes():
    """
    è·å–ç”³ä¸‡ä¸€çº§äºŒçº§æ¿å—æŒ‡æ•°ä»£ç 
    ä»swæ•°æ®è¡¨è·å–Level==1,==2çš„æŒ‡æ•°ä»£ç ï¼Œå¹¶å°†åç¼€.SIæ”¹ä¸º.ZS
    """
    conn = get_db_connection()
    
    # æŸ¥è¯¢ç”³ä¸‡ä¸€çº§äºŒçº§æŒ‡æ•°ä»£ç 
    query = """
    SELECT index_code, industry_name, level
    FROM sw 
    WHERE level IN ('L1', 'L2')
    ORDER BY level, industry_name
    """
    
    df = pd.read_sql_query(query, conn)
    conn.close()
    
    if df.empty:
        print("âš ï¸ æœªæ‰¾åˆ°ç”³ä¸‡ä¸€çº§äºŒçº§æŒ‡æ•°æ•°æ®")
        return pd.DataFrame()
    
    # å°†åç¼€.SIæ”¹ä¸º.ZS
    df['index_code_zs'] = df['index_code'].str.replace('.SI', '.ZS')
    
    print(f"âœ… è·å–åˆ°ç”³ä¸‡æŒ‡æ•°æ•°æ®: {len(df)} æ¡")
    print(f"ğŸ“Š ä¸€çº§æŒ‡æ•°: {len(df[df['level'] == 'L1'])} ä¸ª")
    print(f"ğŸ“Š äºŒçº§æŒ‡æ•°: {len(df[df['level'] == 'L2'])} ä¸ª")
    
    return df


def get_index_price_data(index_codes, start_date, end_date):
    """
    ä»index_k_dailyè¡¨è·å–ç”³ä¸‡æŒ‡æ•°è¡Œæƒ…æ•°æ®
    
    Args:
        index_codes: æŒ‡æ•°ä»£ç åˆ—è¡¨
        start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
    
    Returns:
        pd.DataFrame: æŒ‡æ•°è¡Œæƒ…æ•°æ®
    """
    from data_management.database_manager import DatabaseManager
    db_manager = DatabaseManager()
    
    # æ„å»ºæŸ¥è¯¢æ¡ä»¶
    index_codes_str = "', '".join(index_codes)
    
    query = f"""
    SELECT index_code, trade_date, close
    FROM index_k_daily 
    WHERE index_code IN ('{index_codes_str}')
      AND trade_date >= '{start_date}'
      AND trade_date <= '{end_date}'
    ORDER BY index_code, trade_date
    """
    
    df = db_manager.execute_query(query)
    
    if df.empty:
        print("âš ï¸ æœªæ‰¾åˆ°æŒ‡æ•°è¡Œæƒ…æ•°æ®")
        return pd.DataFrame()
    
    print(f"âœ… è·å–åˆ°æŒ‡æ•°è¡Œæƒ…æ•°æ®: {len(df)} æ¡è®°å½•")
    return df


def calculate_index_returns(price_data, start_date, end_date):
    """
    è®¡ç®—æ¿å—æŒ‡æ•°æ¶¨å¹…
    
    Args:
        price_data: æŒ‡æ•°è¡Œæƒ…æ•°æ®
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    
    Returns:
        pd.DataFrame: åŒ…å«æ¶¨å¹…è®¡ç®—ç»“æœçš„DataFrame
    """
    if price_data.empty:
        return pd.DataFrame()
    
    # è½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼
    price_data['trade_date'] = pd.to_datetime(price_data['trade_date'])
    
    results = []
    
    for index_code in price_data['index_code'].unique():
        index_data = price_data[price_data['index_code'] == index_code].copy()
        index_data = index_data.sort_values(by='trade_date')
        
        if len(index_data) < 2:
            continue
        
        # è·å–å¼€å§‹å’Œç»“æŸä»·æ ¼
        start_price = index_data.iloc[0]['close']
        end_price = index_data.iloc[-1]['close']
        
        # è®¡ç®—æ¶¨å¹…
        if start_price and start_price > 0:
            return_rate = (end_price - start_price) / start_price * 100
        else:
            return_rate = 0
        
        results.append({
            'index_code': index_code,
            'start_date': start_date,
            'end_date': end_date,
            'start_price': start_price,
            'end_price': end_price,
            'return_rate': return_rate,
            'data_points': len(index_data)
        })
    
    return pd.DataFrame(results)


def calculate_sw_sector_returns(start_date, end_date):
    """
    è®¡ç®—ç”³ä¸‡ä¸€çº§äºŒçº§æ¿å—æŒ‡æ•°æ¶¨å¹…çš„ä¸»å‡½æ•°
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
    
    Returns:
        pd.DataFrame: åŒ…å«æ¶¨å¹…è®¡ç®—ç»“æœçš„DataFrame
    """
    print(f"ğŸš€ å¼€å§‹è®¡ç®—ç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
    
    # 1. è·å–ç”³ä¸‡æŒ‡æ•°ä»£ç 
    sw_codes = get_sw_index_codes()
    if sw_codes.empty:
        return pd.DataFrame()
    
    # 2. è·å–æŒ‡æ•°è¡Œæƒ…æ•°æ®
    index_codes_zs = sw_codes['index_code_zs'].tolist()
    price_data = get_index_price_data(index_codes_zs, start_date, end_date)
    
    if price_data.empty:
        return pd.DataFrame()
    
    # 3. è®¡ç®—æ¶¨å¹…
    returns = calculate_index_returns(price_data, start_date, end_date)
    
    if returns.empty:
        return pd.DataFrame()
    
    # 4. åˆå¹¶æŒ‡æ•°ä¿¡æ¯
    result = returns.merge(
        sw_codes[['index_code_zs', 'industry_name', 'level']], 
        left_on='index_code', 
        right_on='index_code_zs', 
        how='left'
    )
    
    # 5. æ’åºå’Œæ ¼å¼åŒ–
    result = result.sort_values('return_rate', ascending=False)
    result['return_rate'] = result['return_rate'].round(2)
    
    print(f"âœ… è®¡ç®—å®Œæˆï¼Œå…± {len(result)} ä¸ªæ¿å—")
    
    return result


def calculate_sw_sector_returns_with_bias(start_date, end_date):
    """
    è®¡ç®—ç”³ä¸‡ä¸€çº§äºŒçº§æ¿å—æŒ‡æ•°æ¶¨å¹…çš„ä¸»å‡½æ•°ï¼ˆå¢åŠ 120BIASå­—æ®µï¼‰
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
    
    Returns:
        pd.DataFrame: åŒ…å«æ¶¨å¹…è®¡ç®—ç»“æœçš„DataFrameï¼Œå¢åŠ 120BIASå­—æ®µ
    """
    print(f"ğŸš€ å¼€å§‹è®¡ç®—ç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…ï¼ˆå«120BIASï¼‰")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
    
    # 1. è·å–ç”³ä¸‡æŒ‡æ•°ä»£ç 
    sw_codes = get_sw_index_codes()
    if sw_codes.empty:
        return pd.DataFrame()
    
    # 2. è·å–æŒ‡æ•°è¡Œæƒ…æ•°æ®ï¼ˆéœ€è¦æ›´å¤šå†å²æ•°æ®æ¥è®¡ç®—120BIASï¼‰
    # ä¸ºäº†è®¡ç®—120BIASï¼Œéœ€è¦è·å–end_dateå‰è‡³å°‘120ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
    index_codes_zs = sw_codes['index_code_zs'].tolist()
    
    # æ‰©å±•æŸ¥è¯¢èŒƒå›´ï¼Œè·å–è¶³å¤Ÿçš„å†å²æ•°æ®
    extended_start_date = pd.to_datetime(end_date) - pd.Timedelta(days=200)  # å¤šè·å–ä¸€äº›æ•°æ®ç¡®ä¿æœ‰120ä¸ªäº¤æ˜“æ—¥
    extended_start_date = extended_start_date.strftime('%Y-%m-%d')
    
    price_data = get_index_price_data(index_codes_zs, extended_start_date, end_date)
    
    if price_data.empty:
        return pd.DataFrame()
    
    # 3. è®¡ç®—æ¶¨å¹…
    returns = calculate_index_returns(price_data, start_date, end_date)
    
    if returns.empty:
        return pd.DataFrame()
    
    # 4. è®¡ç®—120BIAS
    bias_results = []
    
    for index_code in price_data['index_code'].unique():
        index_data = price_data[price_data['index_code'] == index_code].copy()
        index_data = index_data.sort_values(by='trade_date')
        
        if len(index_data) < 120:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è®¡ç®—120BIAS
            continue
        
        # è·å–end_dateå½“å¤©çš„æ”¶ç›˜ä»·
        end_date_data = index_data[index_data['trade_date'] == end_date]
        if end_date_data.empty:
            continue
        
        end_price = end_date_data.iloc[0]['close']
        
        # è®¡ç®—120æ—¥ç§»åŠ¨å¹³å‡
        if len(index_data) >= 120:
            # è·å–æœ€è¿‘120ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
            recent_120_data = index_data.tail(120)
            ma_120 = recent_120_data['close'].mean()
            
            # è®¡ç®—120BIAS
            bias_120 = (end_price - ma_120) / ma_120 * 100 if ma_120 > 0 else 0
            
            bias_results.append({
                'index_code': index_code,
                '120BIAS': round(bias_120, 2)
            })
    
    # 5. åˆå¹¶æ¶¨å¹…å’ŒBIASæ•°æ®
    bias_df = pd.DataFrame(bias_results)
    if bias_df.empty:
        print("âš ï¸ æœªè®¡ç®—å‡º120BIASæ•°æ®")
        return pd.DataFrame()
    
    # åˆå¹¶æ¶¨å¹…å’ŒBIASæ•°æ®
    result = returns.merge(bias_df, on='index_code', how='left')
    
    # 6. åˆå¹¶æŒ‡æ•°ä¿¡æ¯
    result = result.merge(
        sw_codes[['index_code_zs', 'industry_name', 'level']], 
        left_on='index_code', 
        right_on='index_code_zs', 
        how='left'
    )
    
    # 7. æ’åºå’Œæ ¼å¼åŒ–
    result = result.sort_values('return_rate', ascending=False)
    result['return_rate'] = result['return_rate'].round(2)
    
    print(f"âœ… è®¡ç®—å®Œæˆï¼Œå…± {len(result)} ä¸ªæ¿å—ï¼ˆå«120BIASï¼‰")
    
    return result


# def main():
#     """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
#     # è®¾å®šèµ·æ­¢æ—¶é—´
#     start_date = "2024-01-01"
#     end_date = "2024-12-31"
    
#     # è®¡ç®—ç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…
#     results = calculate_sw_sector_returns(start_date, end_date)
    
#     if not results.empty:
#         print("\nğŸ“ˆ ç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…æ’è¡Œæ¦œ:")
#         print("=" * 80)
        
#         # æ˜¾ç¤ºå‰20å
#         top_results = results.head(20)
#         for idx, row in top_results.iterrows():
#             print(f"{row['level']} | {row['industry_name']:<20} | {row['return_rate']:>8.2f}% | {row['index_code']}")
        
#         # ä¿å­˜ç»“æœåˆ°CSV
#         # output_file = f"ç”³ä¸‡æ¿å—æ¶¨å¹…_{start_date}_{end_date}.csv"
#         output_file = f"databases/ç”³ä¸‡æ¿å—æ¶¨å¹…_{start_date}_{end_date}.csv"
#         results.to_csv(output_file, index=False, encoding='utf-8-sig')
#         print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
#         # ç»Ÿè®¡ä¿¡æ¯
#         print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
#         print(f"æ€»æ¿å—æ•°: {len(results)}")
#         print(f"ä¸€çº§æ¿å—: {len(results[results['level'] == 'L1'])}")
#         print(f"äºŒçº§æ¿å—: {len(results[results['level'] == 'L2'])}")
#         print(f"å¹³å‡æ¶¨å¹…: {results['return_rate'].mean():.2f}%")
#         print(f"æœ€å¤§æ¶¨å¹…: {results['return_rate'].max():.2f}%")
#         print(f"æœ€å°æ¶¨å¹…: {results['return_rate'].min():.2f}%")
#     else:
#         print("âŒ æœªè·å–åˆ°è®¡ç®—ç»“æœ")


def run_multiple_periods_analysis_changxian():
    """è¿è¡Œå¤šæ—¶é—´æ®µç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…åˆ†æ"""
    # è®¾å®šå¤šä¸ªèµ·æ­¢æ—¶é—´æ®µ
    time_periods = {
        "2025å¹´1-4æœˆ": ("2025-01-10", "2025-04-11"),
        "2024å¹´9æœˆ-2025å¹´1æœˆ": ("2024-09-13", "2025-01-03"),
        "2024å¹´2-6æœˆ": ("2024-02-02", "2024-06-07")
    }
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {}
    
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæ—¶é—´æ®µçš„ç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…
    for period_name, (start_date, end_date) in time_periods.items():
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æ­£åœ¨è®¡ç®—æ—¶é—´æ®µ: {period_name}")
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
        print(f"{'='*60}")
        
        # è®¡ç®—ç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…ï¼ˆå«120BIASï¼‰
        results = calculate_sw_sector_returns_with_bias(start_date, end_date)
        
        if not results.empty:
            # ä¿å­˜ç»“æœ
            all_results[period_name] = results
            
            # æ˜¾ç¤ºå‰10åï¼ˆå«120BIASï¼‰
            print(f"\nğŸ“ˆ {period_name} ç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…å‰10åï¼ˆå«120BIASï¼‰:")
            print("-" * 120)
            print(f"{'çº§åˆ«':<4} | {'æ¿å—åç§°':<20} | {'æ¶¨å¹…(%)':>8} | {'120BIAS(%)':>10} | {'æŒ‡æ•°ä»£ç '}")
            print("-" * 120)
            top_results = results.head(10)
            for idx, row in top_results.iterrows():
                bias_120 = row.get('120BIAS', 0)
                print(f"{row['level']:<4} | {row['industry_name']:<20} | {row['return_rate']:>8.2f}% | {bias_120:>10.2f}% | {row['index_code']}")
            
            # ä¿å­˜ç»“æœåˆ°CSV
            output_file = f"databases/xunhuan_changxian/ç”³ä¸‡æ¿å—é•¿çº¿æ¶¨å¹…_{period_name}_{start_date}_{end_date}.csv"
            results.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            print(f"\nğŸ“Š {period_name} ç»Ÿè®¡ä¿¡æ¯:")
            print(f"æ€»æ¿å—æ•°: {len(results)}")
            print(f"ä¸€çº§æ¿å—: {len(results[results['level'] == 'L1'])}")
            print(f"äºŒçº§æ¿å—: {len(results[results['level'] == 'L2'])}")
            print(f"å¹³å‡æ¶¨å¹…: {results['return_rate'].mean():.2f}%")
            print(f"æœ€å¤§æ¶¨å¹…: {results['return_rate'].max():.2f}%")
            print(f"æœ€å°æ¶¨å¹…: {results['return_rate'].min():.2f}%")
            
            # 120BIASç»Ÿè®¡ä¿¡æ¯
            if '120BIAS' in results.columns:
                bias_data = results['120BIAS'].dropna()
                if not bias_data.empty:
                    print(f"å¹³å‡120BIAS: {bias_data.mean():.2f}%")
                    print(f"æœ€å¤§120BIAS: {bias_data.max():.2f}%")
                    print(f"æœ€å°120BIAS: {bias_data.min():.2f}%")
                    
                    # æ˜¾ç¤º120BIASæå€¼æ¿å—
                    print(f"\nğŸ“Š 120BIASæå€¼æ¿å—:")
                    max_bias = results.loc[results['120BIAS'].idxmax()]
                    min_bias = results.loc[results['120BIAS'].idxmin()]
                    print(f"æœ€å¤§120BIAS: {max_bias['industry_name']} ({max_bias['120BIAS']:.2f}%)")
                    print(f"æœ€å°120BIAS: {min_bias['industry_name']} ({min_bias['120BIAS']:.2f}%)")
        else:
            print(f"âŒ {period_name} æœªè·å–åˆ°è®¡ç®—ç»“æœ")
    
    # æ±‡æ€»æ‰€æœ‰ç»“æœ
    if all_results:
        print(f"\n{'='*60}")
        print("ğŸ“Š æ‰€æœ‰æ—¶é—´æ®µæ±‡æ€»ç»Ÿè®¡:")
        print(f"{'='*60}")
        
        for period_name, results in all_results.items():
            print(f"{period_name}:")
            print(f"  å¹³å‡æ¶¨å¹…: {results['return_rate'].mean():.2f}%")
            print(f"  æœ€å¤§æ¶¨å¹…: {results['return_rate'].max():.2f}%")
            if '120BIAS' in results.columns:
                bias_data = results['120BIAS'].dropna()
                if not bias_data.empty:
                    print(f"  å¹³å‡120BIAS: {bias_data.mean():.2f}%")
                    print(f"  æœ€å¤§120BIAS: {bias_data.max():.2f}%")
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_file = "databases/ç”³ä¸‡æ¿å—é•¿çº¿æ¶¨å¹…æ±‡æ€».csv"
        with pd.ExcelWriter(summary_file.replace('.csv', '.xlsx'), engine='openpyxl') as writer:
            for period_name, results in all_results.items():
                # æ¸…ç†sheetåç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                sheet_name = period_name.replace('/', '-').replace('å¹´', '').replace('æœˆ', '')
                results.to_excel(writer, sheet_name=sheet_name, index=False)
        
        print(f"\nğŸ’¾ æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {summary_file.replace('.csv', '.xlsx')}")
    
    return all_results




def run_multiple_periods_analysis_zhongji():
    """è¿è¡Œå¤šæ—¶é—´æ®µç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…åˆ†æ"""
    # è®¾å®šå¤šä¸ªèµ·æ­¢æ—¶é—´æ®µ
    time_periods = {
        "2025å¹´6-7æœˆ": ("2025-06-20", "2025-07-31"),  # ä¿®æ”¹ç»“æŸæ—¥æœŸä¸ºæ•°æ®åº“æœ€æ–°æ—¥æœŸ
        "2025å¹´8æœˆ": ("2025-08-01", "2025-09-04"),
    }
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {}
    
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæ—¶é—´æ®µçš„ç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…
    for period_name, (start_date, end_date) in time_periods.items():
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æ­£åœ¨è®¡ç®—æ—¶é—´æ®µ: {period_name}")
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
        print(f"{'='*60}")
        
        # è®¡ç®—ç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…ï¼ˆå«120BIASï¼‰
        results = calculate_sw_sector_returns_with_bias(start_date, end_date)
        
        if not results.empty:
            # ä¿å­˜ç»“æœ
            all_results[period_name] = results
            
            # æ˜¾ç¤ºå‰10åï¼ˆå«120BIASï¼‰
            print(f"\nğŸ“ˆ {period_name} ç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…å‰10åï¼ˆå«120BIASï¼‰:")
            print("-" * 120)
            print(f"{'çº§åˆ«':<4} | {'æ¿å—åç§°':<20} | {'æ¶¨å¹…(%)':>8} | {'120BIAS(%)':>10} | {'æŒ‡æ•°ä»£ç '}")
            print("-" * 120)
            top_results = results.head(10)
            for idx, row in top_results.iterrows():
                bias_120 = row.get('120BIAS', 0)
                print(f"{row['level']:<4} | {row['industry_name']:<20} | {row['return_rate']:>8.2f}% | {bias_120:>10.2f}% | {row['index_code']}")
            
            # ä¿å­˜ç»“æœåˆ°CSV
            output_dir = "databases/xunhuan_zhongji"
            os.makedirs(output_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
            output_file = f"{output_dir}/ç”³ä¸‡æ¿å—ä¸­çº§æ¶¨å¹…_{period_name}_{start_date}_{end_date}.csv"
            results.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            print(f"\nğŸ“Š {period_name} ç»Ÿè®¡ä¿¡æ¯:")
            print(f"æ€»æ¿å—æ•°: {len(results)}")
            print(f"ä¸€çº§æ¿å—: {len(results[results['level'] == 'L1'])}")
            print(f"äºŒçº§æ¿å—: {len(results[results['level'] == 'L2'])}")
            print(f"å¹³å‡æ¶¨å¹…: {results['return_rate'].mean():.2f}%")
            print(f"æœ€å¤§æ¶¨å¹…: {results['return_rate'].max():.2f}%")
            print(f"æœ€å°æ¶¨å¹…: {results['return_rate'].min():.2f}%")
            
            # 120BIASç»Ÿè®¡ä¿¡æ¯
            if '120BIAS' in results.columns:
                bias_data = results['120BIAS'].dropna()
                if not bias_data.empty:
                    print(f"å¹³å‡120BIAS: {bias_data.mean():.2f}%")
                    print(f"æœ€å¤§120BIAS: {bias_data.max():.2f}%")
                    print(f"æœ€å°120BIAS: {bias_data.min():.2f}%")
                    
                    # æ˜¾ç¤º120BIASæå€¼æ¿å—
                    print(f"\nğŸ“Š 120BIASæå€¼æ¿å—:")
                    max_bias = results.loc[results['120BIAS'].idxmax()]
                    min_bias = results.loc[results['120BIAS'].idxmin()]
                    print(f"æœ€å¤§120BIAS: {max_bias['industry_name']} ({max_bias['120BIAS']:.2f}%)")
                    print(f"æœ€å°120BIAS: {min_bias['industry_name']} ({min_bias['120BIAS']:.2f}%)")
        else:
            print(f"âŒ {period_name} æœªè·å–åˆ°è®¡ç®—ç»“æœ")
    
    # æ±‡æ€»æ‰€æœ‰ç»“æœ
    if all_results:
        print(f"\n{'='*60}")
        print("ğŸ“Š æ‰€æœ‰æ—¶é—´æ®µæ±‡æ€»ç»Ÿè®¡:")
        print(f"{'='*60}")
        
        for period_name, results in all_results.items():
            print(f"{period_name}:")
            print(f"  å¹³å‡æ¶¨å¹…: {results['return_rate'].mean():.2f}%")
            print(f"  æœ€å¤§æ¶¨å¹…: {results['return_rate'].max():.2f}%")
            if '120BIAS' in results.columns:
                bias_data = results['120BIAS'].dropna()
                if not bias_data.empty:
                    print(f"  å¹³å‡120BIAS: {bias_data.mean():.2f}%")
                    print(f"  æœ€å¤§120BIAS: {bias_data.max():.2f}%")
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_file = "databases/ç”³ä¸‡æ¿å—ä¸­çº§æ¶¨å¹…æ±‡æ€».csv"
        with pd.ExcelWriter(summary_file.replace('.csv', '.xlsx'), engine='openpyxl') as writer:
            for period_name, results in all_results.items():
                # æ¸…ç†sheetåç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                sheet_name = period_name.replace('/', '-').replace('å¹´', '').replace('æœˆ', '')
                results.to_excel(writer, sheet_name=sheet_name, index=False)
        
        print(f"\nğŸ’¾ æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {summary_file.replace('.csv', '.xlsx')}")
    
    return all_results




def run_multiple_periods_analysis_boduan():
    """è¿è¡Œå¤šæ—¶é—´æ®µç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…åˆ†æ"""
    # è®¾å®šå¤šä¸ªèµ·æ­¢æ—¶é—´æ®µ
    time_periods = {
        "2025å¹´9æœˆ": ("2025-09-04", "2025-09-18"),  # ä¿®æ”¹ç»“æŸæ—¥æœŸä¸ºæ•°æ®åº“æœ€æ–°æ—¥æœŸ
        "2025å¹´8æœˆ": ("2025-08-14", "2025-09-04"),
        "2025å¹´7-8æœˆ": ("2025-07-31", "2025-08-14")
    }
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {}
    
    # å¾ªç¯è®¡ç®—æ¯ä¸ªæ—¶é—´æ®µçš„ç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…
    for period_name, (start_date, end_date) in time_periods.items():
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æ­£åœ¨è®¡ç®—æ—¶é—´æ®µ: {period_name}")
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
        print(f"{'='*60}")
        
        # è®¡ç®—ç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…ï¼ˆå«120BIASï¼‰
        results = calculate_sw_sector_returns_with_bias(start_date, end_date)
        
        if not results.empty:
            # ä¿å­˜ç»“æœ
            all_results[period_name] = results
            
            # æ˜¾ç¤ºå‰10åï¼ˆå«120BIASï¼‰
            print(f"\nğŸ“ˆ {period_name} ç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…å‰10åï¼ˆå«120BIASï¼‰:")
            print("-" * 120)
            print(f"{'çº§åˆ«':<4} | {'æ¿å—åç§°':<20} | {'æ¶¨å¹…(%)':>8} | {'120BIAS(%)':>10} | {'æŒ‡æ•°ä»£ç '}")
            print("-" * 120)
            top_results = results.head(10)
            for idx, row in top_results.iterrows():
                bias_120 = row.get('120BIAS', 0)
                print(f"{row['level']:<4} | {row['industry_name']:<20} | {row['return_rate']:>8.2f}% | {bias_120:>10.2f}% | {row['index_code']}")
            
            # ä¿å­˜ç»“æœåˆ°CSV
            output_file = f"databases/xunhuan_boduan/ç”³ä¸‡æ¿å—æ³¢æ®µæ¶¨å¹…_{period_name}_{start_date}_{end_date}.csv"
            results.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            print(f"\nğŸ“Š {period_name} ç»Ÿè®¡ä¿¡æ¯:")
            print(f"æ€»æ¿å—æ•°: {len(results)}")
            print(f"ä¸€çº§æ¿å—: {len(results[results['level'] == 'L1'])}")
            print(f"äºŒçº§æ¿å—: {len(results[results['level'] == 'L2'])}")
            print(f"å¹³å‡æ¶¨å¹…: {results['return_rate'].mean():.2f}%")
            print(f"æœ€å¤§æ¶¨å¹…: {results['return_rate'].max():.2f}%")
            print(f"æœ€å°æ¶¨å¹…: {results['return_rate'].min():.2f}%")
            
            # 120BIASç»Ÿè®¡ä¿¡æ¯
            if '120BIAS' in results.columns:
                bias_data = results['120BIAS'].dropna()
                if not bias_data.empty:
                    print(f"å¹³å‡120BIAS: {bias_data.mean():.2f}%")
                    print(f"æœ€å¤§120BIAS: {bias_data.max():.2f}%")
                    print(f"æœ€å°120BIAS: {bias_data.min():.2f}%")
                    
                    # æ˜¾ç¤º120BIASæå€¼æ¿å—
                    print(f"\nğŸ“Š 120BIASæå€¼æ¿å—:")
                    max_bias = results.loc[results['120BIAS'].idxmax()]
                    min_bias = results.loc[results['120BIAS'].idxmin()]
                    print(f"æœ€å¤§120BIAS: {max_bias['industry_name']} ({max_bias['120BIAS']:.2f}%)")
                    print(f"æœ€å°120BIAS: {min_bias['industry_name']} ({min_bias['120BIAS']:.2f}%)")
        else:
            print(f"âŒ {period_name} æœªè·å–åˆ°è®¡ç®—ç»“æœ")
    
    # æ±‡æ€»æ‰€æœ‰ç»“æœ
    if all_results:
        print(f"\n{'='*60}")
        print("ğŸ“Š æ‰€æœ‰æ—¶é—´æ®µæ±‡æ€»ç»Ÿè®¡:")
        print(f"{'='*60}")
        
        for period_name, results in all_results.items():
            print(f"{period_name}:")
            print(f"  å¹³å‡æ¶¨å¹…: {results['return_rate'].mean():.2f}%")
            print(f"  æœ€å¤§æ¶¨å¹…: {results['return_rate'].max():.2f}%")
            if '120BIAS' in results.columns:
                bias_data = results['120BIAS'].dropna()
                if not bias_data.empty:
                    print(f"  å¹³å‡120BIAS: {bias_data.mean():.2f}%")
                    print(f"  æœ€å¤§120BIAS: {bias_data.max():.2f}%")
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_file = "databases/ç”³ä¸‡æ¿å—æ³¢æ®µæ¶¨å¹…æ±‡æ€».csv"
        with pd.ExcelWriter(summary_file.replace('.csv', '.xlsx'), engine='openpyxl') as writer:
            for period_name, results in all_results.items():
                # æ¸…ç†sheetåç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                sheet_name = period_name.replace('/', '-').replace('å¹´', '').replace('æœˆ', '')
                results.to_excel(writer, sheet_name=sheet_name, index=False)
        
        print(f"\nğŸ’¾ æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {summary_file.replace('.csv', '.xlsx')}")
    
    return all_results




def analyze_sector_performance():
    """
    åˆ†ææ¿å—æŒ‡æ•°è¡¨ç°
    1. è¯»å–æ¿å—æŒ‡æ•°çš„æ¶¨å¹…CSVæ–‡ä»¶
    2. è·å–æ¶¨å¹…å‰10åçš„æ¿å—åç§°
    3. è·å–120BIASåœ¨quantile(0.5)ä»¥ä¸‹ä¸”æ¶¨å¹…åœ¨quantile(0.5)ä»¥ä¸Šçš„æ¿å—åç§°
    """
    
    # è¯»å–CSVæ–‡ä»¶
    csv_file = r"C:\quant_v2\databases\xunhuan_boduan\ç”³ä¸‡æ¿å—æ³¢æ®µæ¶¨å¹…_2025å¹´7-8æœˆ_2025-07-31_2025-08-14.csv"
    
    try:
        df = pd.read_csv(csv_file, encoding='utf-8-sig')
        print(f"âœ… æˆåŠŸè¯»å–CSVæ–‡ä»¶: {csv_file}")
        print(f"ğŸ“Š æ•°æ®åŒ…å« {len(df)} æ¡è®°å½•")
        print(f"ğŸ“‹ å­—æ®µåˆ—è¡¨: {list(df.columns)}")
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
        return
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
    required_columns = ['industry_name', 'return_rate', '120BIAS']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"âŒ ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
        return
    
    print(f"\n{'='*80}")
    print("ğŸ“ˆ æ¿å—æŒ‡æ•°è¡¨ç°åˆ†æ")
    print(f"{'='*80}")
    
    # 1. è·å–æ¶¨å¹…å‰10åçš„æ¿å—åç§°
    print(f"\nğŸ† æ¶¨å¹…å‰10åçš„æ¿å—:")
    print("-" * 100)
    print(f"{'æ¿å—åç§°':<20} | {'æ¶¨å¹…(%)':>8} | {'120BIAS(%)':>10} | {'çº§åˆ«':<4} | {'æŒ‡æ•°ä»£ç '}")
    print("-" * 100)
    
    # æŒ‰æ¶¨å¹…é™åºæ’åˆ—ï¼Œå–å‰10å
    top_10_by_return = df.nlargest(15, 'return_rate')
    
    for idx, row in top_10_by_return.iterrows():
        level = row.get('level', 'N/A')
        print(f"{row['industry_name']:<20} | {row['return_rate']:>8.2f}% | {row['120BIAS']:>10.2f}% | {level:<4} | {row['index_code']}")
    
    # 2. è®¡ç®—åˆ†ä½æ•°
    return_median = df['return_rate'].quantile(0.6)
    bias_median = df['120BIAS'].quantile(0.4)
    
    print(f"\nğŸ“Š åˆ†ä½æ•°ç»Ÿè®¡:")
    print(f"æ¶¨å¹…ä¸­ä½æ•° (50%åˆ†ä½æ•°): {return_median:.2f}%")
    print(f"120BIASä¸­ä½æ•° (50%åˆ†ä½æ•°): {bias_median:.2f}%")
    
    # 3. è·å–120BIASåœ¨quantile(0.5)ä»¥ä¸‹ä¸”æ¶¨å¹…åœ¨quantile(0.5)ä»¥ä¸Šçš„æ¿å—
    print(f"\nğŸ¯ ç­›é€‰æ¡ä»¶: 120BIAS < {bias_median:.2f}% ä¸” æ¶¨å¹… > {return_median:.2f}%")
    print("-" * 80)
    
    # ç­›é€‰æ¡ä»¶ï¼š120BIAS < ä¸­ä½æ•° ä¸” æ¶¨å¹… > ä¸­ä½æ•°
    filtered_sectors = df[
        (df['120BIAS'] < bias_median) & 
        (df['return_rate'] > return_median)
    ].copy()
    
    if not filtered_sectors.empty:
        # æŒ‰æ¶¨å¹…é™åºæ’åˆ—
        filtered_sectors = filtered_sectors.sort_values('return_rate', ascending=False)
        
        print(f"âœ… æ‰¾åˆ° {len(filtered_sectors)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ¿å—:")
        print(f"{'æ¿å—åç§°':<20} | {'æ¶¨å¹…(%)':>8} | {'120BIAS(%)':>10} | {'çº§åˆ«':<4} | {'æŒ‡æ•°ä»£ç '}")
        print("-" * 100)
        
        for idx, row in filtered_sectors.iterrows():
            level = row.get('level', 'N/A')
            print(f"{row['industry_name']:<20} | {row['return_rate']:>8.2f}% | {row['120BIAS']:>10.2f}% | {level:<4} | {row['index_code']}")
        
        # ä¿å­˜ç­›é€‰ç»“æœ
        output_file = "databases/ç­›é€‰æ¿å—_ä½120BIASé«˜æ¶¨å¹….csv"
        filtered_sectors.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ç­›é€‰ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    else:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ¿å—")
    
    # 4. ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š æ•´ä½“ç»Ÿè®¡ä¿¡æ¯:")
    print(f"æ€»æ¿å—æ•°: {len(df)}")
    print(f"æ¶¨å¹…èŒƒå›´: {df['return_rate'].min():.2f}% ~ {df['return_rate'].max():.2f}%")
    print(f"120BIASèŒƒå›´: {df['120BIAS'].min():.2f}% ~ {df['120BIAS'].max():.2f}%")
    print(f"å¹³å‡æ¶¨å¹…: {df['return_rate'].mean():.2f}%")
    print(f"å¹³å‡120BIAS: {df['120BIAS'].mean():.2f}%")
    
    return {
        'top_10_by_return': top_10_by_return,
        'filtered_sectors': filtered_sectors,
        'return_median': return_median,
        'bias_median': bias_median,
        'all_data': df  # è¿”å›å®Œæ•´æ•°æ®ä¾›åç»­ä½¿ç”¨
    }

def analyze_sector_performance_boduan():
    """
    åˆ†ææ¿å—æŒ‡æ•°è¡¨ç°
    1. è¯»å–æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ¿å—æŒ‡æ•°çš„æ¶¨å¹…CSVæ–‡ä»¶
    2. è·å–æ¶¨å¹…æ’åºå‰15%çš„æ¿å—åç§°æˆ–æ¶¨å¹…20%ä»¥ä¸Šçš„æ¿å—åç§°ï¼ˆæœ€å¤š30ä¸ªï¼‰
    3. è·å–120BIASåœ¨quantile(0.4)ä»¥ä¸‹ä¸”æ¶¨å¹…åœ¨quantile(0.55)ä»¥ä¸Šçš„æ¿å—åç§°
    4ï¼Œåˆ†åˆ«æŒ‰ç…§ä¸Šé¢2ï¼Œ3é¡¹æ¥ä¿å­˜ç­›é€‰ç»“æœã€‚ç”¨dataframeæ¥ä¿å­˜ï¼Œç”¨2å¼ è¡¨ã€‚
    """
    import os
    import glob
    
    # 1. è¯»å–æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ¿å—æŒ‡æ•°çš„æ¶¨å¹…CSVæ–‡ä»¶
    csv_folder = r"C:\quant_v2\databases\xunhuan_boduan"
    csv_files = glob.glob(os.path.join(csv_folder, "*.csv"))
    
    if not csv_files:
        return None
    
    results = {}
    
    for csv_file in csv_files:
        # è·å–æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„å’Œæ‰©å±•åï¼‰
        file_name = os.path.splitext(os.path.basename(csv_file))[0]
    
        try:
            df = pd.read_csv(csv_file, encoding='utf-8-sig')
        except Exception as e:
            continue
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['industry_name', 'return_rate', '120BIAS']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            continue
            
        # 2. è·å–æ¶¨å¹…æ’åºå‰15%çš„æ¿å—åç§°æˆ–æ¶¨å¹…20%ä»¥ä¸Šçš„æ¿å—åç§°ï¼Œä½†æœ€å¤šä¸è¶…è¿‡30ä¸ª
        # æ¶¨å¹…æ’åºå‰15%
        top_15_percent_count = max(1, int(len(df) * 0.15))  # è‡³å°‘1ä¸ª
        top_15_percent_by_return = df.nlargest(top_15_percent_count, 'return_rate').copy()
        
        # æ¶¨å¹…20%ä»¥ä¸Šçš„æ¿å—
        high_return_sectors = df[df['return_rate'] > 20].copy()
        
        # åˆå¹¶ä¸¤ä¸ªæ¡ä»¶ï¼ˆå–å¹¶é›†ï¼Œå»é‡ï¼‰
        table1_sectors = pd.concat([top_15_percent_by_return, high_return_sectors]).drop_duplicates(subset=['industry_name']).copy()
        table1_sectors = table1_sectors.sort_values('return_rate', ascending=False)
        
        # é™åˆ¶æœ€å¤šä¸è¶…è¿‡30ä¸ª
        if len(table1_sectors) > 30:
            table1_sectors = table1_sectors.head(30)
        
        # 3. è·å–120BIASåœ¨quantile(0.4)ä»¥ä¸‹ä¸”æ¶¨å¹…åœ¨quantile(0.55)ä»¥ä¸Šçš„æ¿å—åç§°
        bias_threshold = df['120BIAS'].quantile(0.4)  # 120BIASçš„40%åˆ†ä½æ•°
        return_threshold = df['return_rate'].quantile(0.55)  # æ¶¨å¹…çš„55%åˆ†ä½æ•°
        
        table2_sectors = df[
            (df['120BIAS'] < bias_threshold) & 
            (df['return_rate'] > return_threshold)
        ].copy()
        table2_sectors = table2_sectors.sort_values('return_rate', ascending=False)
        
        # 4. åˆ†åˆ«ä¿å­˜ç­›é€‰ç»“æœåˆ°2å¼ è¡¨
        # ä¿å­˜è¡¨1ï¼šæ¶¨å¹…æ’åºå‰15%æˆ–æ¶¨å¹…20%ä»¥ä¸Šçš„æ¿å—ï¼ˆæœ€å¤š30ä¸ªï¼‰
        output_file1 = rf"databases\xunhuan_boduan_zf\{file_name}_æ¶¨å¹…ç­›é€‰.csv"
        table1_sectors.to_csv(output_file1, index=False, encoding='utf-8-sig')
        
        # ä¿å­˜è¡¨2ï¼šä½120BIASé«˜æ¶¨å¹…æ¿å—
        output_file2 = rf"databases\xunhuan_boduan_bias\{file_name}_biasç­›é€‰.csv"
        table2_sectors.to_csv(output_file2, index=False, encoding='utf-8-sig')
        
        # ä¿å­˜æ¯ä¸ªæ–‡ä»¶çš„ç»“æœ
        results[file_name] = {
            'table1_sectors': table1_sectors,  # æ¶¨å¹…å‰15%æˆ–20%ä»¥ä¸Š
            'table2_sectors': table2_sectors,  # ä½120BIASé«˜æ¶¨å¹…
            'bias_threshold': bias_threshold,
            'return_threshold': return_threshold,
            'all_data': df
        }
    
    return results


def analyze_sector_performance_zhongji():
    """
    åˆ†ææ¿å—æŒ‡æ•°è¡¨ç°
    1. è¯»å–æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ¿å—æŒ‡æ•°çš„æ¶¨å¹…CSVæ–‡ä»¶
    2. è·å–æ¶¨å¹…æ’åºå‰15%çš„æ¿å—åç§°æˆ–æ¶¨å¹…20%ä»¥ä¸Šçš„æ¿å—åç§°ï¼ˆæœ€å¤š30ä¸ªï¼‰
    3. è·å–120BIASåœ¨quantile(0.4)ä»¥ä¸‹ä¸”æ¶¨å¹…åœ¨quantile(0.55)ä»¥ä¸Šçš„æ¿å—åç§°
    4ï¼Œåˆ†åˆ«æŒ‰ç…§ä¸Šé¢2ï¼Œ3é¡¹æ¥ä¿å­˜ç­›é€‰ç»“æœã€‚ç”¨dataframeæ¥ä¿å­˜ï¼Œç”¨2å¼ è¡¨ã€‚
    """
    import os
    import glob
    
    # 1. è¯»å–æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ¿å—æŒ‡æ•°çš„æ¶¨å¹…CSVæ–‡ä»¶
    csv_folder = r"C:\quant_v2\databases\xunhuan_zhongji"
    csv_files = glob.glob(os.path.join(csv_folder, "*.csv"))
    
    if not csv_files:
        return None
    
    results = {}
    
    for csv_file in csv_files:
        # è·å–æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„å’Œæ‰©å±•åï¼‰
        file_name = os.path.splitext(os.path.basename(csv_file))[0]
    
        try:
            df = pd.read_csv(csv_file, encoding='utf-8-sig')
        except Exception as e:
            continue
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['industry_name', 'return_rate', '120BIAS']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            continue
            
        # 2. è·å–æ¶¨å¹…æ’åºå‰15%çš„æ¿å—åç§°æˆ–æ¶¨å¹…20%ä»¥ä¸Šçš„æ¿å—åç§°ï¼Œä½†æœ€å¤šä¸è¶…è¿‡30ä¸ª
        # æ¶¨å¹…æ’åºå‰15%
        top_15_percent_count = max(1, int(len(df) * 0.15))  # è‡³å°‘1ä¸ª
        top_15_percent_by_return = df.nlargest(top_15_percent_count, 'return_rate').copy()
        
        # æ¶¨å¹…20%ä»¥ä¸Šçš„æ¿å—
        high_return_sectors = df[df['return_rate'] > 20].copy()
        
        # åˆå¹¶ä¸¤ä¸ªæ¡ä»¶ï¼ˆå–å¹¶é›†ï¼Œå»é‡ï¼‰
        table1_sectors = pd.concat([top_15_percent_by_return, high_return_sectors]).drop_duplicates(subset=['industry_name']).copy()
        table1_sectors = table1_sectors.sort_values('return_rate', ascending=False)
        
        # é™åˆ¶æœ€å¤šä¸è¶…è¿‡30ä¸ª
        if len(table1_sectors) > 30:
            table1_sectors = table1_sectors.head(30)
        
        # 3. è·å–120BIASåœ¨quantile(0.4)ä»¥ä¸‹ä¸”æ¶¨å¹…åœ¨quantile(0.55)ä»¥ä¸Šçš„æ¿å—åç§°
        bias_threshold = df['120BIAS'].quantile(0.4)  # 120BIASçš„40%åˆ†ä½æ•°
        return_threshold = df['return_rate'].quantile(0.55)  # æ¶¨å¹…çš„55%åˆ†ä½æ•°
        
        table2_sectors = df[
            (df['120BIAS'] < bias_threshold) & 
            (df['return_rate'] > return_threshold)
        ].copy()
        table2_sectors = table2_sectors.sort_values('return_rate', ascending=False)
        
        # 4. åˆ†åˆ«ä¿å­˜ç­›é€‰ç»“æœåˆ°2å¼ è¡¨
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs("databases/xunhuan_zhongji_zf", exist_ok=True)
        os.makedirs("databases/xunhuan_zhongji_bias", exist_ok=True)
        
        # ä¿å­˜è¡¨1ï¼šæ¶¨å¹…æ’åºå‰15%æˆ–æ¶¨å¹…20%ä»¥ä¸Šçš„æ¿å—ï¼ˆæœ€å¤š30ä¸ªï¼‰
        output_file1 = rf"databases\xunhuan_zhongji_zf\{file_name}_æ¶¨å¹…ç­›é€‰.csv"
        table1_sectors.to_csv(output_file1, index=False, encoding='utf-8-sig')
        
        # ä¿å­˜è¡¨2ï¼šä½120BIASé«˜æ¶¨å¹…æ¿å—
        output_file2 = rf"databases\xunhuan_zhongji_bias\{file_name}_biasç­›é€‰.csv"
        table2_sectors.to_csv(output_file2, index=False, encoding='utf-8-sig')
        
        # ä¿å­˜æ¯ä¸ªæ–‡ä»¶çš„ç»“æœ
        results[file_name] = {
            'table1_sectors': table1_sectors,  # æ¶¨å¹…å‰15%æˆ–20%ä»¥ä¸Š
            'table2_sectors': table2_sectors,  # ä½120BIASé«˜æ¶¨å¹…
            'bias_threshold': bias_threshold,
            'return_threshold': return_threshold,
            'all_data': df
        }
    
    return results


def analyze_sector_performance_changxian():
    """
    åˆ†ææ¿å—æŒ‡æ•°è¡¨ç°
    1. è¯»å–æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ¿å—æŒ‡æ•°çš„æ¶¨å¹…CSVæ–‡ä»¶
    2. è·å–æ¶¨å¹…æ’åºå‰15%çš„æ¿å—åç§°æˆ–æ¶¨å¹…20%ä»¥ä¸Šçš„æ¿å—åç§°ï¼ˆæœ€å¤š30ä¸ªï¼‰
    3ï¼Œåˆ†åˆ«æŒ‰ç…§ä¸Šé¢2é¡¹æ¥ä¿å­˜ç­›é€‰ç»“æœã€‚ç”¨dataframeæ¥ä¿å­˜ã€‚
    """
    import os
    import glob
    
    # 1. è¯»å–æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ¿å—æŒ‡æ•°çš„æ¶¨å¹…CSVæ–‡ä»¶
    csv_folder = r"C:\quant_v2\databases\xunhuan_changxian"
    csv_files = glob.glob(os.path.join(csv_folder, "*.csv"))
    
    if not csv_files:
        return None
    
    results = {}
    
    for csv_file in csv_files:
        # è·å–æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„å’Œæ‰©å±•åï¼‰
        file_name = os.path.splitext(os.path.basename(csv_file))[0]
    
        try:
            df = pd.read_csv(csv_file, encoding='utf-8-sig')
        except Exception as e:
            continue
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['industry_name', 'return_rate', '120BIAS']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            continue
            
        # 2. è·å–æ¶¨å¹…æ’åºå‰15%çš„æ¿å—åç§°æˆ–æ¶¨å¹…20%ä»¥ä¸Šçš„æ¿å—åç§°ï¼Œä½†æœ€å¤šä¸è¶…è¿‡30ä¸ª
        # æ¶¨å¹…æ’åºå‰15%
        top_15_percent_count = max(1, int(len(df) * 0.15))  # è‡³å°‘1ä¸ª
        top_15_percent_by_return = df.nlargest(top_15_percent_count, 'return_rate').copy()
        
        # æ¶¨å¹…20%ä»¥ä¸Šçš„æ¿å—
        high_return_sectors = df[df['return_rate'] > 20].copy()
        
        # åˆå¹¶ä¸¤ä¸ªæ¡ä»¶ï¼ˆå–å¹¶é›†ï¼Œå»é‡ï¼‰
        table1_sectors = pd.concat([top_15_percent_by_return, high_return_sectors]).drop_duplicates(subset=['industry_name']).copy()
        table1_sectors = table1_sectors.sort_values('return_rate', ascending=False)
        
        # é™åˆ¶æœ€å¤šä¸è¶…è¿‡30ä¸ª
        if len(table1_sectors) > 30:
            table1_sectors = table1_sectors.head(30)
        
        # 3. è·å–120BIASåœ¨quantile(0.4)ä»¥ä¸‹ä¸”æ¶¨å¹…åœ¨quantile(0.55)ä»¥ä¸Šçš„æ¿å—åç§°
        bias_threshold = df['120BIAS'].quantile(0.4)  # 120BIASçš„40%åˆ†ä½æ•°
        return_threshold = df['return_rate'].quantile(0.55)  # æ¶¨å¹…çš„55%åˆ†ä½æ•°
        
        table2_sectors = df[
            (df['120BIAS'] < bias_threshold) & 
            (df['return_rate'] > return_threshold)
        ].copy()
        table2_sectors = table2_sectors.sort_values('return_rate', ascending=False)
        
        # 4. åˆ†åˆ«ä¿å­˜ç­›é€‰ç»“æœåˆ°2å¼ è¡¨
        # ä¿å­˜è¡¨1ï¼šæ¶¨å¹…æ’åºå‰15%æˆ–æ¶¨å¹…20%ä»¥ä¸Šçš„æ¿å—ï¼ˆæœ€å¤š30ä¸ªï¼‰
        output_file1 = os.path.join("databases", "xunhuan_changxian_zf", f"{file_name}_æ¶¨å¹…ç­›é€‰.csv")
        table1_sectors.to_csv(output_file1, index=False, encoding='utf-8-sig')
        
        # # ä¿å­˜è¡¨2ï¼šä½120BIASé«˜æ¶¨å¹…æ¿å—
        # output_file2 = rf"databases\xunhuan_changxian_bias\{file_name}_biasç­›é€‰.csv"
        # table2_sectors.to_csv(output_file2, index=False, encoding='utf-8-sig')
        
        # ä¿å­˜æ¯ä¸ªæ–‡ä»¶çš„ç»“æœ
        results[file_name] = {
            'table1_sectors': table1_sectors,  # æ¶¨å¹…å‰15%æˆ–20%ä»¥ä¸Š
            'table2_sectors': table2_sectors,  # ä½120BIASé«˜æ¶¨å¹…
            'bias_threshold': bias_threshold,
            'return_threshold': return_threshold,
            'all_data': df
        }
    
    return results



def get_sector_index_codes(result_data, sector_names=None):
    """
    è·å–æŒ‡å®šæ¿å—çš„index_code
    
    Args:
        result_data: analyze_sector_performance()çš„è¿”å›ç»“æœ
        sector_names: æ¿å—åç§°åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›æ‰€æœ‰æ¿å—
    
    Returns:
        dict: {æ¿å—åç§°: index_code}
    """
    if result_data is None or 'all_data' not in result_data:
        print("âŒ æ— æ•ˆçš„ç»“æœæ•°æ®")
        return {}
    
    df = result_data['all_data']
    
    if sector_names is None:
        # è¿”å›æ‰€æœ‰æ¿å—çš„index_code
        sector_codes = dict(zip(df['industry_name'], df['index_code']))
    else:
        # è¿”å›æŒ‡å®šæ¿å—çš„index_code
        sector_codes = {}
        for sector_name in sector_names:
            matching_rows = df[df['industry_name'] == sector_name]
            if not matching_rows.empty:
                sector_codes[sector_name] = matching_rows.iloc[0]['index_code']
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°æ¿å—: {sector_name}")
    
    return sector_codes


if __name__ == "__main__":
    # è¿è¡Œå¤šæ—¶é—´æ®µç”³ä¸‡æ¿å—æŒ‡æ•°æ¶¨å¹…åˆ†æï¼ˆå«120BIASï¼‰
    # run_multiple_periods_analysis_changxian()
    # run_multiple_periods_analysis_zhongji()
    # run_multiple_periods_analysis_boduan()
    #==============================================


    # result = analyze_sector_performance()
    
    # # ç¤ºä¾‹ï¼šè·å–æ¶¨å¹…å‰10åæ¿å—çš„index_code
    # if result:
    #     print(f"\n{'='*80}")
    #     print("ğŸ”— æ¶¨å¹…å‰10åæ¿å—çš„index_code:")
    #     print(f"{'='*80}")
        
    #     top_10_codes = get_sector_index_codes(result, result['top_10_by_return']['industry_name'].tolist())
    #     for sector_name, index_code in top_10_codes.items():
    #         print(f"{sector_name:<20} | {index_code}")
        
    #     # ç¤ºä¾‹ï¼šè·å–ç­›é€‰åæ¿å—çš„index_code
    #     if not result['filtered_sectors'].empty:
    #         print(f"\nğŸ”— ç­›é€‰æ¿å—çš„index_code:")
    #         print("-" * 50)
    #         filtered_codes = get_sector_index_codes(result, result['filtered_sectors']['industry_name'].tolist())
    #         for sector_name, index_code in filtered_codes.items():
    #             print(f"{sector_name:<20} | {index_code}")
    #========================================
    result = analyze_sector_performance_boduan()
    if result:
        print("æ³¢æ®µåˆ†æç»“æœ:")
        for file_name, data in result.items():
            print(f"\næ–‡ä»¶: {file_name}")
            print(f"æ¶¨å¹…å‰15%æˆ–20%ä»¥ä¸Šæ¿å—æ•°é‡: {len(data['table1_sectors'])}")
            print(f"ä½120BIASé«˜æ¶¨å¹…æ¿å—æ•°é‡: {len(data['table2_sectors'])}")
            if not data['table1_sectors'].empty:
                print("æ¶¨å¹…å‰15%æˆ–20%ä»¥ä¸Šæ¿å—:")
                print(data['table1_sectors'][['industry_name', 'return_rate', '120BIAS']].head())
            if not data['table2_sectors'].empty:
                print("ä½120BIASé«˜æ¶¨å¹…æ¿å—:")
                print(data['table2_sectors'][['industry_name', 'return_rate', '120BIAS']].head())
    else:
        print("âŒ æ³¢æ®µåˆ†æå¤±è´¥æˆ–æ²¡æœ‰æ•°æ®")
    result1 = analyze_sector_performance_changxian()
    #========================================
    # è·å–é•¿çº¿æ¶¨å¹…å±…å‰çš„æ¿å—index_codeåˆ—è¡¨
    longxian_changxian_bankuai = get_boduan_bias_bankuai()
    print(longxian_changxian_bankuai)
    aa = analyze_sector_performance_zhongji()
    print(aa)
    
    # print(zhuli_bankuai)  # æ³¨é‡Šæ‰æœªå®šä¹‰çš„å˜é‡


    #è‡ªè®¾æ–°æˆç«‹çš„æ¿å—ã€‚
    #è®¾å®šèµ·æ­¢æ—¶é—´ï¼Œæ¥è®¡ç®—ç”³ä¸‡ä¸€çº§äºŒçº§æ¿å—æŒ‡æ•°çš„æ¶¨å¹…ã€‚ï¼ˆç”³ä¸‡æŒ‡æ•°è¡Œæƒ…æ•°æ®åœ¨index_k_dailyè¡¨ä¸­ã€‚è·å–ç”³ä¸‡ä¸€çº§äºŒçº§æŒ‡æ•°ä»£ç çš„æ–¹å¼ï¼Œé€šè¿‡swæ•°æ®è¡¨ï¼Œç›´æ¥çœ‹Level==1,==2ï¼Œä½†éœ€è¦æŠŠå¯¹åº”çš„index_codeæŠŠåç¼€.SIæ”¹ä¸º.ZSï¼‰